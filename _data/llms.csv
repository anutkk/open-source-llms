Model Name,Size in B-parameters,Release (YY/MM),Publication,Weights,Author,Architecture,English-only/Multilingual,Commercial use (Don't trust me ask a lawyer!),Random Comment,5-shot MMLU accuracy,BBH Score
T0pp,11,21/10,<a href='https://arxiv.org/abs/2110.08207'>Paper</a>,<a href='https://huggingface.co/bigscience/T0pp'>Model</a>,BigScience,Encoder-Decoder,English,<a href='https://github.com/bigscience-workshop/t-zero/blob/master/LICENSE'>✅</a>,First open-source real competitor to GPT-3. Change my mind!,33.7,13.0
T0_3B,3,21/10,<a href='https://arxiv.org/abs/2110.08207'>Paper</a>,<a href='https://huggingface.co/bigscience/T0_3B'>Model</a>,BigScience,Encoder-Decoder,English,<a href='https://github.com/bigscience-workshop/t-zero/blob/master/LICENSE'>✅</a>,Like T0pp but smaller and not better
T0,11,21/10,<a href='https://arxiv.org/abs/2110.08207'>Paper</a>,<a href='https://huggingface.co/bigscience/T0'>Model</a>,BigScience,Encoder-Decoder,English,<a href='https://github.com/bigscience-workshop/t-zero/blob/master/LICENSE'>✅</a>,Like T0pp but not smaller and not better
OPT-IML-30B,30,22/12,<a href='https://arxiv.org/abs/2212.12017'>Paper</a>,<a href='https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML'>Model</a>,Meta,Decoder-only,English (?),❌,December 2022 is a crazy month,,
OPT-IML-Max-30B,30,22/12,<a href='https://arxiv.org/abs/2212.12017'>Paper</a>,<a href='https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML'>Model</a>,Meta,Decoder-only,English (?),❌,Like OPT-IML-30B but morer tasks,43.2,30.9
GPT-JT,6,22/10,<a href='https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai'>Blog Post</a>,<a href='https://huggingface.co/togethercomputer/GPT-JT-6B-v1'>Model</a>,<a href='https://www.together.xyz/'>Together</a>,Decoder-only,English,❔,They have an <a href='https://huggingface.co/spaces/togethercomputer/GPT-JT'>online demo</a>
GPT-J 6B,6,21/08,"<a href='https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/'>Blog</a>, <a href='https://github.com/kingoflolz/mesh-transformer-jax'>GitHub</a>",<a href='https://huggingface.co/EleutherAI/gpt-j-6B'>Model</a>,EleutherAI,Decoder-only,English,✅,More popular than T0pp for some reason. Go T5!
BLOOMZ,176,22/10,<a href='https://arxiv.org/abs/2211.01786'>Paper</a>,<a href='https://huggingface.co/bigscience/bloomz'>Model</a>,<a href='https://bigscience.huggingface.co/'>BigScience</a>,Decoder-only,Multilingual,<a href='https://huggingface.co/spaces/bigscience/license'>❔</a>,"Run on CPU if you want your eyes to bleed, and on GPU if you want them to explode. Prefer BLOOMZ-mt for non-English prompts."
mT0-xxl,13,22/10,<a href='https://arxiv.org/abs/2211.01786'>Paper</a>,<a href='https://huggingface.co/bigscience/mt0-xxl'>Model</a>,BigScience,Encoder-Decoder,Multilingual,<a href='https://huggingface.co/spaces/bigscience/license'>❔</a>,Prefer mT0-xxl-mt for non-English prompts.
BLOOMZ-mt,176,22/10,<a href='https://arxiv.org/abs/2211.01786'>Paper</a>,<a href='https://huggingface.co/bigscience/bloomz-mt'>Model</a>,<a href='https://bigscience.huggingface.co/'>BigScience</a>,Decoder-only,Multilingual,<a href='https://huggingface.co/spaces/bigscience/license'>❔</a>,Does ''Crosslingual Generalization'' means it also understands Klingon?
mT0-xxl-mt,13,22/10,<a href='https://arxiv.org/abs/2211.01786'>Paper</a>,<a href='https://huggingface.co/bigscience/mt0-xxl-mt'>Model</a>,BigScience,Encoder-Decoder,Multilingual,<a href='https://huggingface.co/spaces/bigscience/license'>❔</a>,I can't undertand how BigScience isn't better-known.
GPT-NeoX-20B,20,22/02,"<a href='https://blog.eleuther.ai/announcing-20b/'>Blog</a>, <a href='https://arxiv.org/abs/2204.06745'>ArXiv</a>","<a href='https://huggingface.co/EleutherAI/gpt-neox-20b/tree/main'>HF</a>, <a href='https://the-eye.eu/public/AI/models/GPT-NeoX-20B/'>The Eye</a>",EleutherAI,Decoder-only,English,✅
RWKV-4 14B,14,22/12,<a href='https://github.com/BlinkDL/RWKV-LM'>GitHub</a>,<a href='https://huggingface.co/BlinkDL/rwkv-4-pile-14b'>HF</a>,BlinkDL/EleutherAI,RNN,English,✅,LSTM's big comeback
FLAN-T5-XXL,11,22/10,<a href='https://arxiv.org/abs/2210.11416'>arXiv</a>,<a href='https://huggingface.co/google/flan-t5-xxl'>HF</a>,Google,Encoder-Decoder,English,✅,OPT-IML paper says MMLU accuracy is only  54.9. Also depends if with or without CoT,55.1,45.3
FLAN-T5-XL,3,22/10,<a href='https://arxiv.org/abs/2210.11416'>arXiv</a>,<a href='https://huggingface.co/google/flan-t5-xxl'>HF</a>,Google,Encoder-Decoder,English,✅,Best value for money at the end of 2022,52.4,41.0
